{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "class PolicyIterationAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.observation_dim = env.observation_space.n\n",
        "        self.actions_variants = np.arange(env.action_space.n)\n",
        "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 1 / len(self.actions_variants))\n",
        "        self.state_values = np.zeros(self.observation_dim)\n",
        "        self.maxNumberOfIterations = 1000\n",
        "        self.theta = 1e-6\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def print_policy(self):\n",
        "        print('Текущая политика:')\n",
        "        pprint(self.policy_probs)\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        for _ in range(self.maxNumberOfIterations):\n",
        "            delta = 0\n",
        "            for state in range(self.observation_dim):\n",
        "                v = 0\n",
        "                for action, action_prob in enumerate(self.policy_probs[state]):\n",
        "                    for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                        v += action_prob * prob * (reward + self.gamma * self.state_values[next_state])\n",
        "                delta = max(delta, abs(self.state_values[state] - v))\n",
        "                self.state_values[state] = v\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        policy_stable = True\n",
        "        for state in range(self.observation_dim):\n",
        "            old_action = np.argmax(self.policy_probs[state])\n",
        "            action_values = np.zeros(len(self.actions_variants))\n",
        "            for action in range(len(self.actions_variants)):\n",
        "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                    action_values[action] += prob * (reward + self.gamma * self.state_values[next_state])\n",
        "            best_action = np.argmax(action_values)\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "            self.policy_probs[state] = np.eye(len(self.actions_variants))[best_action]\n",
        "        return policy_stable\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        iteration = 0\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            if self.policy_improvement():\n",
        "                print(f'Политика стабилизировалась после {iteration} итераций.')\n",
        "                break\n",
        "            iteration += 1\n",
        "\n",
        "def play_agent(agent):\n",
        "    state = agent.env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = np.argmax(agent.policy_probs[state])\n",
        "        state, reward, done, _ = agent.env.step(action)\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Taxi-v3')\n",
        "    agent = PolicyIterationAgent(env)\n",
        "    agent.policy_iteration()\n",
        "    agent.print_policy()\n",
        "    print('Награда агента:', play_agent(agent))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGoC1mSHnCGg",
        "outputId": "7f4c2481-2985-49fc-eff0-fd64627b28b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Политика стабилизировалась после 12 итераций.\n",
            "Текущая политика:\n",
            "array([[0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 1., 0.],\n",
            "       ...,\n",
            "       [0., 1., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0., 0.]])\n",
            "Награда агента: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ]
    }
  ]
}